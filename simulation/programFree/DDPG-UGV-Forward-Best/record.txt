if env.reward >= -3.5:

	agent.learn(is_reward_ascent=True)
agent.memory.get_reward_resort(per=10)


def is_Terminal(self, param=None):
    self.terminal_flag = 0
    if self.time > 8.0:
        print('...time out...')
        self.terminal_flag = 2
        return True
    if self.delta_phi_absolute > 4 * math.pi + deg2rad(0) and dis_two_points([self.x, self.y], [self.initX, self.initY]) <= 1.0:
        print('...转的角度太大了...')
        self.terminal_flag = 1
        return True
    if dis_two_points([self.x, self.y], self.terminal) <= self.miss:
        print('...success...')
        self.terminal_flag = 3
        return True
    if self.is_out():
        # print('...out...')
        # self.terminal_flag = 1
        return False
    return False

REWARD：
r1 = -1  # 常值误差，每运行一步，就 -1

if currentError > nextError + 1e-2:
    r2 = 5
elif 1e-2 + currentError < nextError:
    r2 = -5
else:
    r2 = 0

currentTheta = cal_vector_rad([cex, cey], [math.cos(self.current_state[4]), math.sin(self.current_state[4])])
nextTheta = cal_vector_rad([nex, ney], [math.cos(self.next_state[4]), math.sin(self.next_state[4])])
# print(currentTheta, nextTheta)
if currentTheta > nextTheta + 1e-2:
    r3 = 2
elif 1e-3 + currentTheta < nextTheta:
    r3 = -2
else:
    r3 = 0

'''4. 其他'''
r4 = 0
if self.terminal_flag == 3:
    r4 = 500
if self.terminal_flag == 1:  # 出界
    r4 = -2
'''4. 其他'''
# print('r1=', r1, 'r2=', r2, 'r3=', r3, 'r4=', r4)
self.reward = r1 + (r2 + r3 + r4)

'''network'''
self.actor = ActorNetwork(self.actor_lr, self.state_dim_nn, 128, 128, self.action_dim_nn, name='Actor', chkpt_dir=path)
self.target_actor = ActorNetwork(self.actor_lr, self.state_dim_nn, 128, 128, self.action_dim_nn, name='TargetActor', chkpt_dir=path)

self.critic = CriticNetWork(self.critic_lr, self.state_dim_nn, 128, 128, self.action_dim_nn, name='Critic', chkpt_dir=path)
self.target_critic = CriticNetWork(self.critic_lr, self.state_dim_nn, 128, 128, self.action_dim_nn, name='TargetCritic', chkpt_dir=path)
'''network'''
